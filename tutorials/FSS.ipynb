{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee0d9b1c-9e0a-4fe1-891a-a74450c1dd3b",
   "metadata": {},
   "source": [
    "# Fractions Skill Score (FSS)\n",
    "For an explanation of the FSS, and implementation considerations,\n",
    "see: [Fast calculation of the Fractions Skill Score][fss_ref]\n",
    "\n",
    "[fss_ref]:\n",
    "https://www.researchgate.net/publication/269222763_Fast_calculation_of_the_Fractions_Skill_Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2162e15f-0bbc-4b5a-856d-f70d550c779e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## FSS for a single 2D field\n",
    "FSS is computed over 2D arrays representing the observations & forecasts in the spatial domain. The user has to make sure that the input dimensions correspond to the spatial domain e.g. `lat x lon`. Generally the computation involves sliding a window over the input field(s) and applying a threshold over the fcst and obs values.\n",
    "\n",
    "The resulting binary field is summed up to represent the populace (number of ones/\"true\" values in the window).\n",
    "\n",
    "The resulting 2-D field of rolling sums represents \"_Integral Image_\" of the respective forecast and obeservation fields, which is then aggregated over all the sliding windows to compute the fractions skill score.\n",
    "\n",
    "The FSS is then roughly defined as:\n",
    "```\n",
    "    fss = 1 - sum_w((p_o - p_f)^2) / (sum_w(p_o^2) + sum_w(p_f^2))\n",
    "\n",
    "    where,\n",
    "    p_o: observation populace > threshold, in one window\n",
    "    p_f: forecast populace > threshold, in one window\n",
    "    sum_w: sum over all windows\n",
    "````\n",
    "\n",
    "The implementation details are beyond the scope of this tutorial please refer to, [Fast calculation of the Fractions Skill Score][fss_ref] for more info\n",
    "\n",
    "In summary, computation of a single field requires the following parameters:\n",
    "- forecast 2-D field (in spatial domain)\n",
    "- observations 2-D field (in spatial domain)\n",
    "- window size (width x height): The window size of the sliding window\n",
    "- threshold: To compare the input fields against to generate a binary field\n",
    "- compute method: (optional) currently only `numpy` is supported\n",
    "\n",
    "[fss_ref]:\n",
    "https://www.researchgate.net/publication/269222763_Fast_calculation_of_the_Fractions_Skill_Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d466d1f0-1ce2-4517-8a76-dbabbcf14363",
   "metadata": {},
   "source": [
    "**1. Setup** \n",
    "\n",
    "First let's create some random data for our forecast and observation fields. Let's also try out a few scenarios:\n",
    "```\n",
    "scenario 1: obs distribution = fcst distribution = N(0, 1)\n",
    "scenario 2: fcst distribution biased = N(1, 1)\n",
    "scenario 3: fcst distribution variant = N(0, 2)\n",
    "\n",
    "where N(mu, sigma) = normal distribution with mean = mu and standard deviation = sigma\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36904d44-2047-4b9b-ae73-c7a49abd205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# specify input spatial dimensions\n",
    "num_cols = 600\n",
    "num_rows = 400\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# generate random fields\n",
    "obs = np.random.normal(loc=0.0, scale=1.0, size = (num_rows, num_cols))\n",
    "fcst_1 = np.random.normal(loc=0.0, scale=1.0, size = (num_rows, num_cols))\n",
    "fcst_2 = np.random.normal(loc=1.0, scale=1.0, size = (num_rows, num_cols))\n",
    "fcst_3 = np.random.normal(loc=0.0, scale=2.0, size = (num_rows, num_cols))\n",
    "\n",
    "# print out the different scenarios\n",
    "_summarize = lambda x, field: print(\n",
    "    \"{: >20}: shape={}, mean={:.2f}, stddev={:.2f}\".format(\n",
    "    field, x.shape, np.mean(x), np.std(x)\n",
    "))\n",
    "_summarize(obs, \"observations\")\n",
    "_summarize(fcst_1, \"forecast scenario 1\")\n",
    "_summarize(fcst_2, \"forecast scenario 2\")\n",
    "_summarize(fcst_3, \"forecast scenario 3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de79c5-eb7d-49e1-bb74-469d6ef4295e",
   "metadata": {},
   "source": [
    "**2. Define inputs**\n",
    "\n",
    "We need to now specify the threshold, window size and compute method. For now, lets choose a single window, and threshold. While the current `fss` method doesn't allow for more than 1 threshold and window definition per call, we'll see how calculate multiple thresholds/windows in a later step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb3f97-89f1-4b8d-ba7a-1227d06b8757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scores.probability.fss_impl import FssComputeMethod\n",
    "window_size = (100, 100)  # height * width or row size * col size\n",
    "threshold = 0.5  # arbitrarily chosen\n",
    "compute_method = FssComputeMethod.NUMPY  # Note: you can set this to None for now, since only NUMPY is supported currently\n",
    "\n",
    "print(\"window_size:\", window_size)\n",
    "print(\"threshold:\", threshold)\n",
    "print(\"compute_method:\", compute_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e12430b-f3f9-43b6-bc60-9b1e0b77eb63",
   "metadata": {},
   "source": [
    "**3. Run FSS**\n",
    "\n",
    "Since we only have spatial dims we'll be using `scores.probability.fss_impl.fss_2d_single_field` for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf90715-41bb-4c11-8cf0-2b3d50696088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scores.probability.fss_impl import fss_2d_single_field\n",
    "\n",
    "# compile scenarios\n",
    "scenarios = {\n",
    "    \"scenario 1 (same distribution)\": [obs, fcst_1],\n",
    "    \"scenario 2 (biased fcst)\": [obs, fcst_2],\n",
    "    \"scenario 3 (variant fcst)\": [obs, fcst_3],\n",
    "}\n",
    "result = []\n",
    "\n",
    "# run through each scenario and compute FSS with inputs defined above\n",
    "for s, v in scenarios.items():\n",
    "    _obs, _fcst = v\n",
    "    _fss = fss_2d_single_field(\n",
    "        _fcst,\n",
    "        _obs,\n",
    "        event_threshold=threshold,\n",
    "        window_size=window_size,\n",
    "        compute_method=compute_method,\n",
    "    )\n",
    "    result.append((s, _fss))\n",
    "\n",
    "# tabulate results\n",
    "print(f\"{' '*30} | fss score\")\n",
    "print(f\"{' '*30} | ---------\")\n",
    "_ = [print(\"{:<30} | {}\".format(s, v)) for (s, v) in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f1dcb6-9b48-4394-9aa7-b1f1911897fb",
   "metadata": {},
   "source": [
    "As apparent above, with the same distribution, we get a score close to 1, this is because the FSS doesn't actually care about where in any given windows the binary fields match; only the total count. With a biased distribution the score dips a lot, which is expected with a threshold of 0.5 and a bias of 1.0. Whereas for a variant forecast, we still get a reasonable score, this is also expected since the variation isn't too large and the distributions still overlap quite a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e1c13-4f25-4a19-9b76-593e2968f348",
   "metadata": {},
   "source": [
    "**4. Multiple inputs**\n",
    "\n",
    "Suppose now that we want to collate data for multiple thresholds/windows. There are several ways of doing this, including vectorization. The following will show one way of doing it that, while more verbose, would hopefully help decompose the operations required to create the final accumulated dataset.\n",
    "\n",
    "Now that we understand how the argument mapping works, let's re-create the mapping and run the fss, we'll also store the results in a `W x T x S` array before converting it to xarray for displaying.\n",
    "```\n",
    "W x T x S where,\n",
    "W = number of windows\n",
    "T = number of thresholds\n",
    "S = number of scenarios\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b44931-accc-4024-a51a-bf766811234e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# as before\n",
    "window_sizes = np.linspace((20,20), (100,100), 5, dtype=(int,int))\n",
    "thresholds = np.linspace(0.0, 1.0, 5, dtype=float)\n",
    "input_scenarios = [[obs, fcst_1], [obs, fcst_2], [obs, fcst_3]]\n",
    "compute_method = FssComputeMethod.NUMPY\n",
    "\n",
    "# create output array\n",
    "W = len(window_sizes)\n",
    "T = len(thresholds)\n",
    "S = len(input_scenarios)\n",
    "fss_out = np.zeros((W, T, S))\n",
    "\n",
    "# lets iterate over the indices, setting the `multi_index` and `writeonly` flags\n",
    "with np.nditer(fss_out, flags=['multi_index'], op_flags=['writeonly']) as it:\n",
    "    for _fss in it:\n",
    "        # gather argument indices\n",
    "        window_idx, threshold_idx, input_idx = it.multi_index\n",
    "        _window_size = window_sizes[window_idx]\n",
    "        _threshold = thresholds[threshold_idx]\n",
    "        _obs, _fcst = input_scenarios[input_idx]\n",
    "        # compute the fss for each combination of arguments\n",
    "        _fss[...] = fss_2d_single_field(\n",
    "            _fcst,\n",
    "            _obs,\n",
    "            event_threshold=_threshold,\n",
    "            window_size=_window_size,\n",
    "            compute_method=compute_method\n",
    "        )\n",
    "\n",
    "# construct output xarray with results\n",
    "da = xr.DataArray(\n",
    "    data=fss_out,\n",
    "    dims=[\"window_size\", \"threshold\", \"scenario\"],\n",
    "    coords=dict(\n",
    "        window_size=[str(x) for x in window_sizes],\n",
    "        threshold=[str(x) for x in thresholds],\n",
    "        scenario=range(len(input_scenarios))\n",
    "    ),\n",
    "    attrs=dict(\n",
    "        description=\"Fractions skill score\",\n",
    "    ),\n",
    ")\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c11724-e10e-49e3-af69-3ef6f8b96ffd",
   "metadata": {},
   "source": [
    "## Aggregated FSS\n",
    "\n",
    "The aggregated FSS groups a list of 2-D spatial fields from the input data array and applies the single 2D FSS in the previous section over each field. It then accumulates the decomposed scores into the final score. A description of the algorithm is provided at the bottom of this tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8aca2d-b043-4551-92e8-1829307d11ab",
   "metadata": {},
   "source": [
    "**1. Setup** \n",
    "\n",
    "Similar to the previous section lets start by setting up our inputs. This time we will include the scenarios within the input source itself. Let's define our inputs:\n",
    "\n",
    "```\n",
    "Fcst\n",
    "----\n",
    "Dims: T x L x N x M\n",
    "\n",
    "Obs\n",
    "---\n",
    "Dims: T x N x M\n",
    "\n",
    "where,\n",
    "  T = time_idx [0..24]\n",
    "  L = lead_time_idx [0..6]\n",
    "  N = lat_idx [0..400]\n",
    "  M = lon_idx [0..600]\n",
    "```\n",
    "\n",
    "We will also assume that the variance and bias increase as the lead time increases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35534a7-d8b2-418d-9319-96a160ae3f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# lets start by defining a function to generate a T x N x M array since this will be commonly used\n",
    "T = 24\n",
    "L = 6\n",
    "N = 400\n",
    "M = 600\n",
    "# set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "def _generate_input(dims, mu, sigma):\n",
    "    return np.random.normal(loc=mu, scale=sigma, size = dims)\n",
    "\n",
    "# let's generate obs using the standard normal distribution N(0,1) for a T x N x M field\n",
    "obs = _generate_input((T, N, M), 0.0, 1.0)\n",
    "\n",
    "# now for the fcst we'll need to generate and stack the leadtime arrays\n",
    "# since we only have 6 lead times we can be a bit lazy and use a for loop\n",
    "# we will also increase the bias and variance as we increase the leadtime\n",
    "var_inc = 0.5\n",
    "bias_inc = 0.25\n",
    "var, bias = 1.0, 0.0\n",
    "fcst = []\n",
    "for i in range(L):\n",
    "    var += var_inc\n",
    "    bias += bias_inc\n",
    "    fcst.append(\n",
    "        _generate_input((T, N, M), bias, var)\n",
    "    )\n",
    "fcst = np.stack(fcst)    \n",
    "\n",
    "# we now have the arrays in raw numpy format, we'll need some annotations\n",
    "# lets convert it to xarray and check that our final dimensions match\n",
    "da_obs = xr.DataArray(\n",
    "    data=obs,\n",
    "    dims=[\"time_idx\", \"lat_idx\", \"lon_idx\"],\n",
    "    coords=dict(\n",
    "        time_idx=range(T),\n",
    "        lat_idx=range(N),\n",
    "        lon_idx=range(M),\n",
    "    ),\n",
    "    attrs=dict(\n",
    "        description=\"observations\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "da_fcst = xr.DataArray(\n",
    "    data=fcst,\n",
    "    dims=[\"lead_time_idx\", \"time_idx\", \"lat_idx\", \"lon_idx\"],\n",
    "    coords=dict(\n",
    "        lead_time_idx=range(L),\n",
    "        time_idx=range(T),\n",
    "        lat_idx=range(N),\n",
    "        lon_idx=range(M),\n",
    "    ),\n",
    "    attrs=dict(\n",
    "        description=\"forecast\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c812ef-d4ee-461b-ba1d-0c09ed056190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check fcst coordinates\n",
    "da_fcst.coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42e611-6da7-4d7f-99f4-e9aa938290aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check obs coordinates\n",
    "da_obs.coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d1d70a-cee3-4d34-99b2-438a5052e82a",
   "metadata": {},
   "source": [
    "**2. Run Aggregate FSS**\n",
    "\n",
    "We choose accumulation dimensions and run FSS. Here we want to preserve the `lead_time`, and calculate the `fss` reduced over space (`lat`, `lon`) followed by `time`.\n",
    "\n",
    "Here we use `scores.probability.fss_impl.fss_2d` which automatically tries to compute the decomposed `fss` score for each dimension reduced over space using `scores.probability.fss_impl.fss_2d_single_field` similar to the previous section.\n",
    "\n",
    "Additionally it can take in `preserve_dims` or (mutually exclusive) `reduce_dims` function arguments to determine which dimensions to aggregate over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a08e2-7074-4149-b1fd-8cb3c315adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scores.probability.fss_impl import fss_2d\n",
    "\n",
    "# As before we currently only support a single threshold/window argument.\n",
    "threshold = 0.5\n",
    "window_size = (100, 100)\n",
    "\n",
    "\n",
    "# since our input arrays already encapculate the various scenarios in the time/lead time dimensions,\n",
    "# we just need to provide the inputs\n",
    "\n",
    "# IMPORTANT: Note that we need to provide the names of the spatial_dims explicitly, unlike in the previous\n",
    "# section, where the input is assumed to be a 2D spatial field, this function doesn't know which dimensions\n",
    "# are spatial unelss you tell it.\n",
    "fss_out = fss_2d(\n",
    "    da_fcst,\n",
    "    da_obs,\n",
    "    event_threshold=threshold,\n",
    "    window_size=window_size,\n",
    "    spatial_dims=[\"lat_idx\", \"lon_idx\"],\n",
    "    preserve_dims=[\"lead_time_idx\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a806f8f-c960-4b97-8c5f-6dc7428c2174",
   "metadata": {},
   "outputs": [],
   "source": [
    "fss_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5767cf98-db72-4194-ae70-ae8ef74cfe1c",
   "metadata": {},
   "source": [
    "**Result**\n",
    "\n",
    "We should now see a dataset of FSS values with only `lead_time_idx` as the preserved dimension. If we look at the data, the FSS decreases as the leadtime increases, because we made our simulated data distribution shift away from the obs as the lead time increased. So this roughly looks like what we'd expect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f85ad8d-882b-470e-93e5-3fde82af937b",
   "metadata": {},
   "source": [
    "**Exercise - multiple windows/thresholds:** An exercise is left to the reader to implement a similar method to support multiple input args for `fss_2d`. See: \"4. Multiple Inputs\" in the previous section for an idea. Note that this version would be slightly trickier since the result of `fss_2d` is not necessarily a scalar, however, there are still many approaches to solving this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4227292c-769a-4062-b43f-c3325bc43e0d",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "**Accumulated FSS Algorithm**\n",
    "\n",
    "```\n",
    "Let's assume our data set is of the form lat x lon x time x lead_time => N x M x T x L\n",
    "where,\n",
    "    lat x lon = spatial dims\n",
    " \n",
    "1. [Map] Map the input field into a list of fields to be aggregated over. i.e.\n",
    "\n",
    "        lat x lon x time x lead_time -> [lat x lon] x lead_time,\n",
    "        where,\n",
    "            [lat x lon] is a list of T spatial fields.\n",
    "            keep_dim = lead_time; spatial_dims = lat, lon; collapse_dim = time\n",
    "\n",
    "2. [Compute] For each lead time compute the decomposed fss score for [lat x lon] (len = T).\n",
    "    \n",
    "        The decomposed FSS score is a tuple containing the components required\n",
    "        to formulate the inal score:\n",
    "        \n",
    "        (obs_sum / T, fcst_sum / T, (obs_sum - fcst_sum) / T)\n",
    "\n",
    "        The division by T is optional, but good to have for larger datasets to\n",
    "        avoid integer overflow issues when aggregating.\n",
    "\n",
    "3. [Reduce] Sum the decomposed score across the accumulated dimension.\n",
    "\n",
    "        fss <- [0; 1 x L]\n",
    "        for each leadtime, lt {\n",
    "            obs_sum <- sum_T(fss_decomposed[lt][0])\n",
    "            fcst_sum <- sum_T(fss_decomposed[lt][1])\n",
    "            diff_sum <- sum_T(fss_decomposed[lt][2])\n",
    "            fss[lt] <- dff_sum / (obs_sum + fcst_sum)\n",
    "        }\n",
    "        return fss :: size := 1 x L\n",
    "\n",
    "\n",
    "The reason why we need to keep track of the decomposed score instead of simply\n",
    "aggregating the final score is because:\n",
    "\n",
    "1/x + 1/y != 1 / (x+y)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
